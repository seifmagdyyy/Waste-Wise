#@title Step 1: Mount Google Drive and set up the environment
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import os

# Mount your Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Set the path to your dataset folder in Google Drive.
# Make sure that inside this folder you have the sub-folders "PLastic", "Metal", and "Paper".
# Update the following path as necessary.
dataset_dir = '/content/drive/MyDrive/Colab_Notebooks/Grad_Project/Dataset_New'  # <-- change this to your folder path

# Print sub-folders (should show your 3 classes)
print("Found classes:", os.listdir(dataset_dir))

#@title Step 2: Create Training and Validation Datasets
# Define image parameters and batch size
img_height = 640
img_width = 640
batch_size = 16

# Use 80% of the data for training and 20% for validation.
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    dataset_dir,
    validation_split=0.2,
    subset="training",
    seed=123,  # ensure shuffling is consistent between subsets
    image_size=(img_height, img_width),
    batch_size=batch_size,
)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    dataset_dir,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size,
)

# Get class names (should be ["Plastic", "Metal", "Paper"] or similar, depending on alphabetical order)
print("Class names:", train_ds.class_names)

#@title Step 3: Set Up Data Augmentation and Performance Optimization
# Create a data augmentation pipeline.
data_augmentation = keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
])

# Optimize dataset performance with caching and prefetching.
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds   = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

#@title Step 4: Build the Model with Transfer Learning (MobileNetV2)
# Load the pre-trained MobileNetV2 model, excluding the top classification layers.
base_model = tf.keras.applications.MobileNetV2(
    input_shape=(img_height, img_width, 3),
    include_top=False,
    weights='imagenet'
)
base_model.trainable = False  # Freeze the base model for initial training

# Build the classification head on top of the base model.
inputs = keras.Input(shape=(img_height, img_width, 3))
x = data_augmentation(inputs)  # Apply data augmentation
# Preprocess the inputs using MobileNetV2's preprocessing function.
x = tf.keras.applications.mobilenet_v2.preprocess_input(x)
x = base_model(x, training=False)  # Ensure the base model runs in inference mode
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.2)(x)  # Regularization
# Final output layer with 3 classes; adjust the number of units if you have more/less classes.
outputs = layers.Dense(3, activation='softmax')(x)
model = keras.Model(inputs, outputs)

# Compile the model with an optimizer, loss, and metric.
model.compile(optimizer=keras.optimizers.Adam(),
              loss='sparse_categorical_crossentropy',  # Note: our labels are integer encoded
              metrics=['accuracy'])

# Display the model architecture.
model.summary()


#@title Step 5: Train the Model
# Set the number of epochs. You can increase this number for improved performance (training time will increase).
epochs = 25

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs

#@title Step 6: Visualize Training Results
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs_range = range(epochs)

plt.figure(figsize=(12, 6))

# Plot training and validation accuracy.
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy', marker='o')
plt.plot(epochs_range, val_acc, label='Validation Accuracy', marker='o')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

# Plot training and validation loss.
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss', marker='o')
plt.plot(epochs_range, val_loss, label='Validation Loss', marker='o')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()


#Convert model to tensor lite
keras_model = tf.keras.models.load_model('/content/drive/MyDrive/Colab_Notebooks/Grad_Project/Waste_Wise_Model.keras')

converter = tf.lite.TFLiteConverter.from_keras_model(keras_model) 
tflite_model = converter.convert()

with open('converted_model.tflite', 'wb') as f:     
  f.write(tflite_model)

converter.optimizations = [tf.lite.Optimize.DEFAULT] 
tflite_model_quantized = converter.convert()

